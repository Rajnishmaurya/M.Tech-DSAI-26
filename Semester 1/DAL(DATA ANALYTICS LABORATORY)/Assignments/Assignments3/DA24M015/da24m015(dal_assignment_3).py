# -*- coding: utf-8 -*-
"""DA24M015(DAL Assignment 3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kVuTG7sxx1gg1f3fLs847LIMaFtStrZB
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

from google.colab import drive
drive.mount('/content/drive')

!ls "/content/drive/My Drive"

data=pd.read_csv('/content/drive/My Drive/Colab Notebooks/Assignment3.csv')

data

"""Statistics of the data"""

data.describe()

"""Check for any null values"""

data.isna().sum()

"""Visualization of the relationsip of the features using pairplot"""

import seaborn as sns

sns.pairplot(data)

"""**Observations 1:**
1. As we can observe from the pairplot that only features x1 and features x4 are having linear relationship with dependent features.
2. Feature x2, feature x3 and feature x5 not have linear relationship with the dependent features.

# Task 1
Fit OLS on the data directly and evaluate the baseline SSE loss. You will observe that the loss is very high, but that’s ok. You will strive hard to apply creative ways to reduce the loss.
"""

# separating the dependent and independent features
y=data['y']    # independent features
X=data.drop(columns=['y'])  # independent features

y    # dependent feature

X  #  independent features

import statsmodels.api as sm

"""Model Summary"""

X=sm.add_constant(X)
model=sm.OLS(y,X).fit()
print(model.summary())

"""Here R squared and adjusted R squared are same.

Can we have the same Rsquared and AdjRsquared ?
  * Yes,it can be same in two case:
  1. Single predictor: When there is only one independent variable. the R squared and adjusted R square will be the same because the adjustment factor for k becomes negligible.
  2. Perfect fit: If the model perfectly predicts the dependent variable(R squared=1),the R squared and adjusted R squared will be same.

Sum of squared Errors(SSE) and Root Mean Squared Error(RMSE)
"""

predictions=model.predict(X)
residuals=y-predictions
## SSE
sse1=np.sum(residuals**2)
rmse1=np.sqrt(sse1/101)
print("Sum of Squared Errors(SSE):",sse1)
print("Root Mean Squared Error(RMSE):",rmse1)

"""# Task 2
Perform EDA on the dataset to understand the predictor features and how are they influencing each other. Also, study how each individual predictor influence the output variable. You may use correlation study to estimate the influence. Add necessary visualization and its representive interpretations to substantiate your inferences. The outcome of this step is figure out the requires features and their respective transformation.

Statistics of the features and outputs
"""

# description of the data in a DataFrame. The description includes summary statistics for each numerical column in the DataFrame,
# such as the number of non-empty values, the mean, the standard deviation, the minimum and maximum values, and the percentiles
data.describe()

"""Check for any null values"""

data.isna().sum()

"""From above two(.describe() and .isna().sum()) observations we can see that there is no null values in the dataset

Bivariate Analysis Using Correlation Matrix
"""

correlation_matrix = data.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

"""**Observations 2:**
1. Features 1 and feature 4 is highly positive correlated , so we can drop one feature among feautre 1 and feature 4.
2. Features 3 and feature 5 is highly positive correlated , so we can drop one feature among feautre 3 and feature 5.
3. Independent feature1(x1) and dependent feature y, independent feature 4 and dependent feature y is highly positive correlated.
4. Independent feature(x2 and x5) and y is positively correlated.
5. Independent feature(x3) and dependent feature y is negatively correlated.
"""

# Variance Inflation Factor
from statsmodels.stats.outliers_influence import variance_inflation_factor
import statsmodels.api as sm

vif=pd.DataFrame()
vif['Variables']=X.columns
vif['vif']=[variance_inflation_factor(X.values,i) for i in range(X.shape[1])]

vif

"""Variance Inflation Factor(VIF):
* It measures how much the variance of a regression coefficient is inflated due to multicollinearity with other variables.
* VIF=1/(1-R**2) , where R**2 is used to calculate the accuracy of the model.
* VIF=1 No multicollinearity(the variables is not correlated with other predictors)
* 1<VIF<5: Moderate correlation.
* VIF>=5 Indicates problematic multicollinearity.
* VIF>=10 String multicollinearity,seen as a serious issues.

**Observations 3:**
1.  We can see that variables x1 and variable x4 are highly correlated.The same oblervation we have seen using the correlation matrix.
2.  We will remove the features const and feature x1.
"""

X=X.drop(columns=['x1','const'])
X

"""Let's see the Correlation Matrix after removing the features x1 from the dataset."""

correlation_matrix = X.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

"""Variance Inflation factor"""

# Variance Inflation Factor
from statsmodels.stats.outliers_influence import variance_inflation_factor
import statsmodels.api as sm

vif=pd.DataFrame()
vif['Variables']=X.columns
vif['vif']=[variance_inflation_factor(X.values,i) for i in range(X.shape[1])]

vif

"""**Observations 4:**
1. We can observe from **correlation matrix** and **variance inflation factor** that there is **no independent features** which are **highly correlated.**

**Conclusion of EDA**
1. From **observation 1** we have observed that feature x2 , feature x3 and feature x5 not have any linear realtionship with the dependent features.
2. So, we add some feature which are having polynomial degree(degree=2) consist of feature x2 ,feature x3 and feature x5.

Adding some extra features
"""

X['x6']=X['x2']*X['x5']
X['x7']=X['x2']*X['x4']
X['x8']=X['x2']*X['x3']
X['x9']=X['x3']*X['x4']
X['x10']=X['x3']*X['x5']
X['x11']=X['x4']*X['x5']
X['x12']=X['x2']*X['x2']
X['x13']=X['x3']*X['x3']
X['x14']=X['x5']*X['x5']
X

"""# Task 3
Fit OLS on the selected and transformed features and check if the loss has reduced from the baseline estimation.
"""

X['y']=y
X

"""Visualization of the relationship of the dataset using pairplot."""

sns.pairplot(X)

X=X.drop(columns=['y'])
X=sm.add_constant(X)
model=sm.OLS(y,X).fit()
print(model.summary())

"""Sum of squared Errors(SSE) and Root Mean Squared Error(RMSE)"""

predictions=model.predict(X)
residuals=y-predictions
sse2=np.sum(residuals**2)
rmse2=np.sqrt(sse2/101)
print("Sum of Squared Errors(SSE):",sse2)
print("Root Mean Squared Error(RMSE):",rmse2)

"""**Observations 5:**
1. SSE is reduced.
2. RMSE is reduced

# Task 4
Install ‘lazypredict’ package and use the LazyRegressor class to build the regression models.Compare the RMSE reported by all the regression models from LazyRegressor against your OLS losses.Infer the reasons for why different techniques report different performance metrics.
"""

from lazypredict.Supervised import LazyRegressor
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=355)

# Initialize LazyRegressor
regressor = LazyRegressor(verbose=0, ignore_warnings=True, custom_metric=None)


# Fit the models on the training data and evaluate them on the test data
models, predictions = regressor.fit(x_train, x_test, y_train, y_test)

# Display the perfomance of the models
print(models)

"""**Observations 6:**
1. RMSE reported by all the regression models using LazyRegressor is in between 12 to 10289.
2. RMSE reported by the task 1 is 26.67698999975848.
2. RMSE reported by the task 3 after performing EDA is 9.376096037258227

**THE END**
"""

